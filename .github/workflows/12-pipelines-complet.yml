# Workflow 12 : Pipeline MLOps Complet
# Objectif : Int√©grer toutes les notions dans un vrai pipeline CI/CD pour ML

name: 12 - Pipeline MLOps Complet

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      force_deploy:
        description: 'Forcer le d√©ploiement m√™me si les tests √©chouent'
        type: boolean
        default: false

env:
  PYTHON_VERSION: '3.10'
  MODEL_NAME: 'classification_model'

jobs:
  # =====================================
  # PHASE 1 : VALIDATION DU CODE
  # =====================================
  code-quality:
    name: üìù Qualit√© du Code
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Installer outils de linting
        run: |
          pip install flake8 black isort
      
      - name: Cr√©er des fichiers Python d'exemple
        run: |
          mkdir -p src
          cat > src/model.py << 'EOF'
          """Module pour l'entra√Ænement de mod√®les ML."""
          import numpy as np
          from sklearn.ensemble import RandomForestClassifier
          
          def train_model(X, y, n_estimators=100):
              """Entra√Æne un mod√®le Random Forest."""
              model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
              model.fit(X, y)
              return model
          
          def evaluate_model(model, X, y):
              """√âvalue un mod√®le."""
              return model.score(X, y)
          EOF
          
          cat > src/preprocessing.py << 'EOF'
          """Module de pr√©traitement des donn√©es."""
          import numpy as np
          from sklearn.preprocessing import StandardScaler
          
          def preprocess_data(X):
              """Normalise les donn√©es."""
              scaler = StandardScaler()
              return scaler.fit_transform(X)
          EOF
      
      - name: Linting avec flake8
        run: |
          flake8 src/ --count --select=E9,F63,F7,F82 --show-source --statistics --exit-zero
          flake8 src/ --count --max-complexity=10 --max-line-length=100 --statistics --exit-zero
      
      - name: V√©rifier le formatage (black)
        run: black --check src/
      
      - name: V√©rifier les imports (isort)
        run: isort --check-only src/

  # =====================================
  # PHASE 2 : TESTS
  # =====================================
  unit-tests:
    name:  Tests Unitaires
    runs-on: ubuntu-latest
    needs: code-quality
    
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      
      - name: Cr√©er requirements.txt
        run: |
          cat > requirements.txt << 'EOF'
          numpy>=1.24.0
          pandas>=2.0.0
          scikit-learn>=1.3.0
          joblib>=1.3.0
          pytest>=7.4.0
          pytest-cov>=4.1.0
          EOF
      
      - name: Installer d√©pendances
        run: |
          pip install -r requirements.txt
      
      - name: Cr√©er les tests
        run: |
          mkdir -p tests
          cat > tests/test_model.py << 'EOF'
          """Tests pour le module model."""
          import pytest
          import numpy as np
          from sklearn.datasets import make_classification
          from sklearn.ensemble import RandomForestClassifier
          
          def test_model_training():
              """Test l'entra√Ænement du mod√®le."""
              X, y = make_classification(n_samples=100, n_features=10, random_state=42)
              model = RandomForestClassifier(n_estimators=10, random_state=42)
              model.fit(X, y)
              assert model.score(X, y) > 0.7
          
          def test_model_predictions():
              """Test les pr√©dictions du mod√®le."""
              X, y = make_classification(n_samples=100, n_features=10, random_state=42)
              model = RandomForestClassifier(n_estimators=10, random_state=42)
              model.fit(X, y)
              predictions = model.predict(X[:5])
              assert len(predictions) == 5
              assert all(p in [0, 1] for p in predictions)
          
          def test_model_shape():
              """Test la forme des donn√©es."""
              X, y = make_classification(n_samples=100, n_features=10, random_state=42)
              assert X.shape == (100, 10)
              assert y.shape == (100,)
          EOF
      
      - name: Ex√©cuter les tests avec couverture
        run: |
          pytest tests/ -v --cov=. --cov-report=term-missing
      
      - name: Afficher r√©sum√©
        run: |
          echo "‚úÖ Tests unitaires pass√©s sur Python ${{ matrix.python-version }}"

  # =====================================
  # PHASE 3 : ENTRA√éNEMENT DU MOD√àLE
  # =====================================
  train-model:
    name:  Entra√Ænement du Mod√®le
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Installer d√©pendances
        run: |
          pip install numpy pandas scikit-learn joblib matplotlib seaborn
      
      - name: Entra√Æner le mod√®le
        id: training
        run: |
          python << 'EOF'
          import os
          import json
          import joblib
          import numpy as np
          import matplotlib.pyplot as plt
          from sklearn.datasets import make_classification
          from sklearn.model_selection import train_test_split, cross_val_score
          from sklearn.ensemble import RandomForestClassifier
          from sklearn.metrics import classification_report, confusion_matrix
          from datetime import datetime
          
          print("=" * 50)
          print("ENTRA√éNEMENT DU MOD√àLE")
          print("=" * 50)
          
          # G√©n√©rer des donn√©es
          print("\n1. G√©n√©ration des donn√©es...")
          X, y = make_classification(
              n_samples=2000,
              n_features=20,
              n_informative=15,
              n_redundant=5,
              random_state=42
          )
          
          X_train, X_test, y_train, y_test = train_test_split(
              X, y, test_size=0.2, random_state=42
          )
          print(f"   Train set: {X_train.shape}")
          print(f"   Test set: {X_test.shape}")
          
          # Entra√Æner le mod√®le
          print("\n2. Entra√Ænement...")
          model = RandomForestClassifier(
              n_estimators=100,
              max_depth=10,
              random_state=42,
              n_jobs=-1
          )
          model.fit(X_train, y_train)
          
          # √âvaluation
          print("\n3. √âvaluation...")
          train_score = model.score(X_train, y_train)
          test_score = model.score(X_test, y_test)
          
          # Cross-validation
          cv_scores = cross_val_score(model, X_train, y_train, cv=5)
          
          print(f"   Train accuracy: {train_score:.4f}")
          print(f"   Test accuracy: {test_score:.4f}")
          print(f"   CV mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
          
          # Sauvegarder les m√©triques
          metrics = {
              'train_accuracy': float(train_score),
              'test_accuracy': float(test_score),
              'cv_mean': float(cv_scores.mean()),
              'cv_std': float(cv_scores.std()),
              'n_samples': int(len(X)),
              'n_features': int(X.shape[1]),
              'timestamp': datetime.now().isoformat()
          }
          
          with open('metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)
          
          # Sauvegarder le mod√®le
          print("\n4. Sauvegarde du mod√®le...")
          joblib.dump(model, 'model.pkl')
          
          # Feature importance plot
          plt.figure(figsize=(10, 6))
          importances = model.feature_importances_
          indices = np.argsort(importances)[::-1][:10]
          plt.bar(range(10), importances[indices])
          plt.xlabel('Feature Index')
          plt.ylabel('Importance')
          plt.title('Top 10 Feature Importances')
          plt.tight_layout()
          plt.savefig('feature_importance.png')
          
          print("\n‚úÖ Entra√Ænement termin√© !")
          print("=" * 50)
          
          # Export pour les steps suivants
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"test_accuracy={test_score}\n")
              f.write(f"model_quality={'good' if test_score > 0.85 else 'poor'}\n")
          EOF
      
      - name: Upload du mod√®le et des m√©triques
        uses: actions/upload-artifact@v4
        with:
          name: model-artifacts
          path: |
            model.pkl
            metrics.json
            feature_importance.png
          retention-days: 30
      
      - name: V√©rifier la qualit√© du mod√®le
        run: |
          if (( $(echo "${{ steps.training.outputs.test_accuracy }} > 0.85" | bc -l) )); then
            echo "‚úÖ Mod√®le de bonne qualit√©: ${{ steps.training.outputs.test_accuracy }}"
          else
            echo "  Mod√®le de qualit√© insuffisante: ${{ steps.training.outputs.test_accuracy }}"
          fi

  # =====================================
  # PHASE 4 : VALIDATION DU MOD√àLE
  # =====================================
  validate-model:
    name:  Validation du Mod√®le
    runs-on: ubuntu-latest
    needs: train-model
    
    steps:
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: T√©l√©charger les artifacts
        uses: actions/download-artifact@v4
        with:
          name: model-artifacts
      
      - name: Installer d√©pendances
        run: pip install numpy scikit-learn joblib
      
      - name: Valider le mod√®le
        run: |
          python << 'EOF'
          import json
          import joblib
          import numpy as np
          
          print("=" * 50)
          print("VALIDATION DU MOD√àLE")
          print("=" * 50)
          
          # Charger les m√©triques
          with open('metrics.json', 'r') as f:
              metrics = json.load(f)
          
          print("\nM√©triques:")
          for key, value in metrics.items():
              print(f"  {key}: {value}")
          
          # Charger le mod√®le
          model = joblib.load('model.pkl')
          print(f"\nMod√®le: {type(model).__name__}")
          print(f"Param√®tres: {model.get_params()}")
          
          # Tests de validation
          print("\nTests de validation:")
          
          # Test 1: Pr√©cision minimale
          test_acc = metrics['test_accuracy']
          assert test_acc > 0.70, f"Pr√©cision trop faible: {test_acc}"
          print(f"  ‚úÖ Test 1: Pr√©cision suffisante ({test_acc:.4f})")
          
          # Test 2: Pas d'overfitting
          train_acc = metrics['train_accuracy']
          overfitting = train_acc - test_acc
          assert overfitting < 0.15, f"Overfitting d√©tect√©: {overfitting}"
          print(f"  ‚úÖ Test 2: Pas d'overfitting ({overfitting:.4f})")
          
          # Test 3: Pr√©dictions valides
          X_test = np.random.randn(10, metrics['n_features'])
          preds = model.predict(X_test)
          assert all(p in [0, 1] for p in preds), "Pr√©dictions invalides"
          print(f"  ‚úÖ Test 3: Pr√©dictions valides")
          
          print("\n‚úÖ Validation r√©ussie !")
          print("=" * 50)
          EOF

  # =====================================
  # PHASE 5 : D√âPLOIEMENT
  # =====================================
  deploy:
    name:  D√©ploiement
    runs-on: ubuntu-latest
    needs: validate-model
    # D√©ployer seulement sur main ou si forc√©
    if: github.ref == 'refs/heads/main' || github.event.inputs.force_deploy == 'true'
    environment: production
    
    steps:
      - name: T√©l√©charger les artifacts
        uses: actions/download-artifact@v4
        with:
          name: model-artifacts
      
      - name: Afficher informations de d√©ploiement
        run: |
          echo "=" * 50
          echo "D√âPLOIEMENT EN PRODUCTION"
          echo "=" * 50
          echo ""
          echo "Repository: ${{ github.repository }}"
          echo "Branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          echo "Actor: ${{ github.actor }}"
          echo ""
          cat metrics.json
          echo ""
          ls -lh
      
      - name: Simuler d√©ploiement
        run: |
          echo "üì¶ Upload du mod√®le vers le service ML..."
          sleep 2
          echo "‚úÖ Mod√®le d√©ploy√© avec succ√®s"
          echo ""
          echo "üîó URL du mod√®le: https://api.exemple.com/models/${{ env.MODEL_NAME }}"
          echo "üìä Dashboard: https://monitoring.exemple.com"

  # =====================================
  # PHASE 6 : NOTIFICATION
  # =====================================
  notify:
    name: üìß Notification
    runs-on: ubuntu-latest
    needs: [deploy, validate-model]
    if: always()
    
    steps:
      - name: R√©sum√© du pipeline
        run: |
          echo "=" * 60
          echo "R√âSUM√â DU PIPELINE MLOPS"
          echo "=" * 60
          echo ""
          echo "‚úÖ Qualit√© du code: OK"
          echo "‚úÖ Tests unitaires: OK"
          echo "‚úÖ Entra√Ænement: OK"
          echo "‚úÖ Validation: OK"
          echo "‚úÖ D√©ploiement: ${{ needs.deploy.result }}"
          echo ""
          echo "Repository: ${{ github.repository }}"
          echo "Workflow: ${{ github.workflow }}"
          echo "Run: #${{ github.run_number }}"
          echo ""
          echo "=" * 60